{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61e91f52-bd26-48bf-8843-c22f686171df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Camada Silver\n",
    "Essa etapa é responsável por limpar, transformar e padronizar os dados recebidos pela etapa bronze.\n",
    "\n",
    "## Objetivos:\n",
    "- Efetuei a leitura dos dados da tabela Delta da Camada Bronze.\n",
    "\n",
    "- Converti os tipos de dados de *String* para *Bigint*, *Double*, *Date*, etc.\n",
    "\n",
    "- Padronizei os valores ausentes (como \"NA\", \"None\", \"N None present, etc\") para *NULL*.\n",
    "\n",
    "- Dedupliquei os registros.\n",
    "\n",
    "- Ingeri os dados na tabela Delta da Camada Silver de forma incremental, usando *MERGE INTO*\n",
    "\n",
    "- A tabela Silver criada está no formato Delta e é gerenciada pelo Unity Catalog.\n",
    "\n",
    "- Ao final comparo a tabela silver e bronze para demonstrar o tratamentro dos valores e deduplicação.\n",
    "\n",
    "## Decisões Técnicas:\n",
    "- Novamente tentei me ater ao paralelismo utilizando o *Spark*.\n",
    "\n",
    "- Adotei ID como chave primária da tabela SQL.\n",
    "\n",
    "- Para tratar os tipos de dados utilizei PySpark e criei um dicionário com o nome das colunas e seus respectivos tipos de dados. Para identificar os tipos certos analisei o CSV.\n",
    "\n",
    "- Esse dataset possui muitos valores nulos de formatos diferentes, seja um campo vazio, \"NA\", \"None\", \"N - None\", etc. Nesse caso analisei o CSV, e busquei as ocorrências na tabela, mesmo assim, acredito que há uma forma mais otimizada de filtrar esses nulos. \n",
    "\n",
    "- Para a deduplicação, filtrei pela chave primário ID, e em seguida utilizei o comando *MERGE INTO* para mesclar os dados apenas se o ID for único.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8886986e-b974-4d55-b555-e3feef8a300e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importando bibliotecas necessárias\n",
    "from pyspark.sql.functions import col, to_date, lit, coalesce, expr, current_timestamp\n",
    "from pyspark.sql.types import LongType, DoubleType, StringType, DateType, TimestampType\n",
    "from pyspark.sql.window import Window \n",
    "\n",
    "# Tabela Bronze fonte\n",
    "tabela_bronze_caminho = \"`workspace`.`default`.`fire_incidents_bronze`\" # Caminho no Unity Catalog\n",
    "tabela_bronze_nome = \"fire_incidents_bronze\"\n",
    "\n",
    "# Tabela Silver destino\n",
    "tabela_silver_caminho = \"`workspace`.`default`.`fire_incidents_silver`\" # Caminho no Unity Catalog\n",
    "tabela_silver_nome = \"fire_incidents_silver\"\n",
    "\n",
    "print(f\"Fonte de dados (Camada Bronze): {tabela_bronze_caminho}\")\n",
    "print(f\"Destino de dados (Camada Silver): {tabela_silver_caminho}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf9e7874-d1c0-443b-987d-afddef634965",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Lendo os dados da Camada Bronze\n",
    "print(f\"\\nLendo dados da Camada Bronze ({tabela_bronze_caminho})...\")\n",
    "\n",
    "bronze_df = spark.table(tabela_bronze_nome)\n",
    "\n",
    "print(\"Schema da Camada Bronze (esperado todo String):\")\n",
    "bronze_df.printSchema()\n",
    "print(f\"Total de registros lidos da Camada Bronze: {bronze_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0bfe3e2-c503-4206-9d0d-ee4e8a9199bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aplicando transformações e limpando os dados\n",
    "\n",
    "from pyspark.sql.functions import col, to_date, lit, coalesce, when, trim, expr\n",
    "from pyspark.sql.types import LongType, DoubleType, StringType, DateType\n",
    "\n",
    "# Dicionário que mapeia o nome da coluna para o tipo PySpark desejado.\n",
    "column_type_mapping = {\n",
    "    \"ID\": LongType(), \n",
    "    \"Incident Number\": LongType(), \n",
    "    \"Exposure Number\": LongType(), \n",
    "    \"Call Number\": LongType(), \n",
    "    \"Suppression Units\": LongType(), \n",
    "    \"Suppression Personnel\": LongType(), \n",
    "    \"EMS Units\": LongType(), \n",
    "    \"EMS Personnel\": LongType(), \n",
    "    \"Other Units\": LongType(), \n",
    "    \"Other Personnel\": LongType(), \n",
    "    \"Fire Fatalities\": LongType(), \n",
    "    \"Fire Injuries\": LongType(), \n",
    "    \"Civilian Fatalities\": LongType(), \n",
    "    \"Civilian Injuries\": LongType(), \n",
    "    \"Number of Alarms\": LongType(), \n",
    "    \"Floor of Fire Origin\": LongType(), \n",
    "    \"Number of floors with minimum damage\": LongType(), \n",
    "    \"Number of floors with significant damage\": LongType(), \n",
    "    \"Number of floors with heavy damage\": LongType(), \n",
    "    \"Number of floors with extreme damage\": LongType(), \n",
    "    \"Number of Sprinkler Heads Operating\": LongType(), \n",
    "    \"Supervisor District\": LongType(), \n",
    "    \"Estimated Property Loss\": DoubleType(), \n",
    "    \"Estimated Contents Loss\": DoubleType(), \n",
    "    \"Incident Date\": DateType()\n",
    "}\n",
    "\n",
    "# Tratando variações de nulo\n",
    "def clean_and_standardize_string(column_name):\n",
    "    column_expr = col(column_name) \n",
    "    return when(\n",
    "        (trim(column_expr) == \"NA\") |\n",
    "        (trim(column_expr) == \"None\") |\n",
    "        (trim(column_expr) == \"N None\") |\n",
    "        (trim(column_expr) == \"N - None\") |\n",
    "        (trim(column_expr) == \"N None present\") |  \n",
    "        (trim(column_expr) == \"N -Not present\") | \n",
    "        (trim(column_expr) == \"null\") | \n",
    "        (trim(column_expr) == \"UU - Undetermined\") |\n",
    "        (trim(column_expr) == \"\") | \n",
    "        column_expr.isNull(),      \n",
    "        lit(None)                  \n",
    "    ).otherwise(column_expr)       \n",
    "select_expressions = []\n",
    "\n",
    "for col_info in bronze_df.schema.fields: # Itera sobre os campos do schema da Bronze\n",
    "    col_name = col_info.name # Nome da coluna, ex: \"Incident Number\"\n",
    "    quoted_col_name = f\"`{col_name}`\" if \" \" in col_name else col_name # Adiciona backticks se necessário\n",
    "\n",
    "    # Aplica a limpeza de strings primeiro a TODAS as colunas\n",
    "    cleaned_col_expr = clean_and_standardize_string(quoted_col_name)\n",
    "\n",
    "    # Verifica se a coluna tem um mapeamento de tipo específico\n",
    "    if col_name in column_type_mapping:\n",
    "        target_type = column_type_mapping[col_name]\n",
    "        \n",
    "        if target_type == DateType():\n",
    "            cast_expr = to_date(cleaned_col_expr, \"yyyy/MM/dd\") # Ajuste o formato da data\n",
    "        elif target_type in [LongType(), DoubleType()]:\n",
    "            # Para numéricos, usa TRY_CAST.\n",
    "            cast_expr = expr(f\"TRY_CAST({quoted_col_name} AS {target_type.simpleString().upper()})\")\n",
    "            if col_name == \"ID\":\n",
    "                cast_expr = coalesce(cast_expr, lit(-1)) # Valor padrão para ID inválido\n",
    "        else: # Colunas mapeadas como StringType, apenas garantem o cast para String\n",
    "            cast_expr = cleaned_col_expr.cast(StringType()) # Garante que o tipo seja String\n",
    "    else: # Se a coluna NÃO está no column_type_mapping, ela é tratada como String\n",
    "        cast_expr = cleaned_col_expr.cast(StringType()) # Garante que o tipo seja String após a limpeza\n",
    "\n",
    "    select_expressions.append(cast_expr.alias(col_name)) \n",
    "    \n",
    "# Criando o DataFrame transformado\n",
    "silver_df = bronze_df.select(*select_expressions)\n",
    "\n",
    "# VAlidando a chave primária 'ID'\n",
    "silver_df = silver_df.filter(col(\"ID\").isNotNull()) \n",
    "\n",
    "print(\"\\nSchema do DataFrame transformado (silver_df - após o tratamento otimizado):\")\n",
    "silver_df.printSchema()\n",
    "print(f\"Total de registros no 'silver_df': {silver_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75756d0b-1f6e-4933-8fc0-17c19182b18b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ingestão Incremental e Deduplicação\n",
    "\n",
    "print(\"\\nRealizando deduplicação e ingestão incremental na Camada Silver...\")\n",
    "\n",
    "# 1. Adicionar o timestamp de carregamento para os dados\n",
    "silver_df_final = silver_df.withColumn(\"silver_loaded_at\", current_timestamp())\n",
    "\n",
    "# 2. Deduplicação por ID (chave primária) - Antes do MERGE\n",
    "# Isso remove duplicatas dentro do lote de dados atual (se o mesmo ID aparecer várias vezes no bronze).\n",
    "print(f\"Registros antes da deduplicação (pelo ID no lote atual): {silver_df_final.count()}\")\n",
    "silver_df_final = silver_df_final.dropDuplicates([\"ID\"])\n",
    "print(f\"Registros após a deduplicação (pelo ID no lote atual): {silver_df_final.count()}\")\n",
    "\n",
    "# 3. Preparar os dados para o MERGE INTO.\n",
    "silver_df_final.createOrReplaceTempView(\"fire_incidents_silver_staging\")\n",
    "print(f\"Total de registros no staging DataFrame para o MERGE: {silver_df_final.count()}\") \n",
    "\n",
    "# 4. Executando o MERGE INTO para a Camada Silver\n",
    "# Excluir a tabela Silver antes de recriar\n",
    "# Isso garante que a tabela seja sempre recriada com o schema correto do silver_df_final\n",
    "print(f\"\\nVerificando e recriando a tabela Silver '{tabela_silver_caminho}' para garantir o schema correto...\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {tabela_silver_caminho}\") \n",
    "# Usamos .limit(0) para criar a tabela APENAS com o schema na primeira execução.\n",
    "# 'delta.columnMapping.mode' é necessário para nomes de colunas com espaços.\n",
    "silver_df_final.limit(0).write \\\n",
    "                      .format(\"delta\") \\\n",
    "                      .mode(\"overwrite\") \\\n",
    "                      .option(\"overwriteSchema\", \"true\") \\\n",
    "                      .option(\"delta.columnMapping.mode\", \"name\") \\\n",
    "                      .saveAsTable(tabela_silver_nome) # saveAsTable registra no Unity Catalog \n",
    "\n",
    "print(f\"Tabela Silver '{tabela_silver_caminho}' criada/recriada com sucesso com o esquema correto (vazia de dados neste passo).\")\n",
    "# Contagem de registros na Camada Silver ANTES do MERGE\n",
    "silver_target_count_before_merge = spark.table(tabela_silver_nome).count()\n",
    "print(f\"Total de registros na Camada Silver ANTES do MERGE: {silver_target_count_before_merge}\")\n",
    "\n",
    "\n",
    "# 5. Executar o MERGE INTO para inserir/atualizar dados. \n",
    "print(f\"\\nIniciando operação MERGE INTO para a tabela Delta '{tabela_silver_caminho}'...\")\n",
    "df_column_names_raw = silver_df_final.columns\n",
    "update_set_clauses = []\n",
    "for col_name in df_column_names_raw:\n",
    "    quoted_col_name = f\"`{col_name}`\" # Adiciona backticks para o SQL\n",
    "    update_set_clauses.append(f\"target.{quoted_col_name} = source.{quoted_col_name}\")\n",
    "update_set_sql_string = \",\\n    \".join(update_set_clauses) \n",
    "# Strings para INSERT (colunas)\n",
    "insert_columns_sql_string = \", \".join([f\"`{col_name}`\" for col_name in df_column_names_raw])\n",
    "# Strings para INSERT (valores)\n",
    "insert_values_sql_string = \", \".join([f\"source.`{col_name}`\" for col_name in df_column_names_raw])\n",
    "spark.sql(f\"\"\"\n",
    "  MERGE INTO {tabela_silver_caminho} AS target\n",
    "  USING fire_incidents_silver_staging AS source\n",
    "  ON target.ID = source.ID \n",
    "  WHEN MATCHED THEN UPDATE SET\n",
    "    {update_set_sql_string}\n",
    "  WHEN NOT MATCHED THEN INSERT (\n",
    "    {insert_columns_sql_string}\n",
    "  )\n",
    "  VALUES (\n",
    "    {insert_values_sql_string}\n",
    "  )\n",
    "\"\"\")\n",
    "print(f\"MERGE INTO concluído para a tabela Delta '{tabela_silver_caminho}'.\")\n",
    "print(\"A Camada Silver foi atualizada com dados limpos e deduplicados.\")\n",
    "\n",
    "# Contagem de registros na Silver depois do MERGE\n",
    "silver_target_count_after_merge = spark.table(tabela_silver_nome).count()\n",
    "print(f\"Total de registros na Camada Silver DEPOIS do MERGE: {silver_target_count_after_merge}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18bca34f-03cc-4a13-b51c-51c3013eaeaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importando bibliotecas necessárias\n",
    "from pyspark.sql.functions import col, to_date, lit, coalesce, expr, current_timestamp\n",
    "from pyspark.sql.types import LongType, DoubleType, StringType, DateType, TimestampType\n",
    "from pyspark.sql.window import Window # Necessário para deduplicação avançada, se aplicável\n",
    "\n",
    "# Tabela Bronze fonte\n",
    "tabela_bronze_caminho = \"`workspace`.`default`.`fire_incidents_bronze`\" # Caminho no Unity Catalog\n",
    "tabela_bronze_nome = \"fire_incidents_bronze\"\n",
    "\n",
    "# Tabela Silver destino\n",
    "tabela_silver_caminho = \"`workspace`.`default`.`fire_incidents_silver`\" # Caminho no Unity Catalog\n",
    "tabela_silver_nome = \"fire_incidents_silver\"\n",
    "\n",
    "print(f\"Fonte de dados (Camada Bronze): {tabela_bronze_caminho}\")\n",
    "print(f\"Destino de dados (Camada Silver): {tabela_silver_caminho}\")\n",
    "\n",
    "\n",
    "# Verificando e Comparação Silver e Bronze\n",
    "print(\"\\nVerificando 10 registros da Camada Silver e comparando com a Bronze\")\n",
    "\n",
    "final_silver_df = spark.table(tabela_silver_nome)\n",
    "\n",
    "# 1. Obtendo os IDs dos primeiros 10 registros da Camada Silver\n",
    "silver_sample_ids_df = final_silver_df.select(\"ID\").limit(10)\n",
    "silver_sample_ids = [row.ID for row in silver_sample_ids_df.collect()]\n",
    "\n",
    "print(f\"\\nIDs de exemplo da Camada Silver para comparação: {silver_sample_ids}\")\n",
    "\n",
    "# 2. Filtrando os mesmos registros na Camada Silver (para mostrar os dados transformados)\n",
    "print(\"\\n--- Registros selecionados na Camada Silver\")\n",
    "silver_records_for_comparison = final_silver_df.filter(col(\"ID\").isin(silver_sample_ids)).orderBy(\"ID\")\n",
    "display(silver_records_for_comparison)\n",
    "\n",
    "# 3. Filtrando os mesmos registros na Camada Bronze (para mostrar os dados originais)\n",
    "print(\"\\n--- Registros correspondentes na Camada Bronze (dados brutos): ---\")\n",
    "bronze_df_raw = spark.table(tabela_bronze_nome)\n",
    "\n",
    "# Convertendo a coluna 'ID' da Bronze para BIGINT ANTES de filtrar.\n",
    "# Isso garante que a comparação seja feita entre tipos numéricos.\n",
    "bronze_records_for_comparison = bronze_df_raw.filter(\n",
    "    expr(\"TRY_CAST(ID AS BIGINT)\").isin(silver_sample_ids)\n",
    ").orderBy(\"ID\")\n",
    "\n",
    "display(bronze_records_for_comparison)\n",
    "\n",
    "print(\"Observe a diferença nos tipos de dados Silver e Bronze, e o tratamento de nulos/formato.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2-Camada_Silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
