{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac940316-510f-46c9-add5-a2eaff23bae8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Camada Bronze\n",
    "Essa etapa é responsável por ingerir os dados do dataset público de incidentes de incêndio de San Francisco\n",
    "\n",
    "## Objetivos:\n",
    "- Ingerir os dados do CSV armazenado a cada execução, atualizando a tabela.\n",
    "\n",
    "- Estruturei a tabela no formato Delta, registrando no Unity Catalog.\n",
    "\n",
    "- A ingestão dos dados é incremental, o comando *COPY INTO* lê o volume com os arquivos CSV e adiciona apenas aqueles que ainda não foram carregados na tabela SQL.\n",
    "\n",
    "## Decisões Técnicas:\n",
    "- Tentei me ater ao paralelismo utilizando o *Spark*.\n",
    "\n",
    "- Criei a tabela pela interface do Databricks, definindo como Delta Lake.\n",
    "\n",
    "- O requisito **\"Ingerir diariamente o dataset\"** me gerou uma dúvida. Identifiquei que o site do dataset atualiza o CSV diariamante, nesse caso, acredito que uma solução de web scraping seria mais eficaz, baixando os novos dados diariamante para a ingestão. Porém, lendo a documentação da versão gratuita do databricks, descobri que o acesso externo à Internet é restrito, nesse caso optei por adicionar manualmente os arquivos CSV no volume \"arquivos_brutos\", simulando a ingestão diaria ao executar as camadas após a inclusão de um novo CSV.\n",
    "\n",
    "- Ao criar a tabela SQL para ingerir os dados do CSV, tive problemas ao usar o comando *COPY INTO* para mesclar os dados utulizando a opção **\"inferSchema\"** como *True*, há algum conflito nos tipos de dados inferidos pela plataforma para o CSV e a tabela SQL. Nesse caso declarei a opção como *False*, e adotei inicialmente o tipo de dado String como padrão para todos os valores, tratando-os posteriormente na camada **Silver**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffbe440c-2fd3-4f59-8aee-a73cd8cb0d7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuração Inicial\n",
    "\n",
    "## Importando bibliotecas necessárias\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "import os\n",
    "\n",
    "csv_input = \"/Volumes/workspace/default/arquivos_brutos/\" # pasta com os arquivos CSV no Unity Catalog\n",
    "tabela_bronze_caminho = \"`workspace`.`default`.`fire_incidents_bronze`\" # Caminho da tabela no Unity Catalog\n",
    "tabela_bronze_nome = \"fire_incidents_bronze\" # Nome para referênciar no PySpark\n",
    "\n",
    "print(f\"Diretório de entrada (Volume do Unity Catalog): {csv_input}\")\n",
    "print(f\"Nome completo da tabela Bronze existente: {tabela_bronze_caminho}\")\n",
    "print(f\"Nome simples da tabela Bronze existente: {tabela_bronze_nome}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "939956fd-3705-4044-b5bf-1e6b43d808e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verificando o Volume\n",
    "\n",
    "print(\"O comando 'COPY INTO' no próximo passo lerá o diretório do Volume para novos arquivos.\")\n",
    "print(f\"\\n Verificando o conteúdo do Volume de entrada: {csv_input}\")\n",
    "try:\n",
    "    # dbutils.fs.ls lista o conteúdo de um caminho no sistema de arquivos do Databricks\n",
    "    volume_contents = dbutils.fs.ls(csv_input)\n",
    "    if not volume_contents:\n",
    "        print(f\"AVISO: O Volume '{csv_input}' está vazio.\")\n",
    "    else:\n",
    "        print(\"Conteúdo encontrado no Volume:\")\n",
    "        for item in volume_contents:\n",
    "            print(f\"- {item.path}\")\n",
    "        print(\"\\nSUCESSO: Arquivos CSV encontrados no Volume. Pronto para a ingestão incremental.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nERRO: Falha ao acessar o Volume '{csv_input}'.\")\n",
    "    print(f\"Detalhes do erro: {e}\")\n",
    "    print(\"Por favor, verifique se o caminho do Volume está correto e se seus arquivos foram carregados para lá.\")\n",
    "    dbutils.notebook.exit(\"Falha na Etapa Bronze: Não foi possível acessar o Volume de entrada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39b0fb3f-d805-4b63-8ea6-ac841e700a9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ingestão de Dados\n",
    "\n",
    "print(f\"Iniciando operação COPY INTO para {tabela_bronze_caminho} a partir do Volume {csv_input}\")\n",
    "spark.sql(f\"\"\"\n",
    "  COPY INTO {tabela_bronze_caminho}\n",
    "  FROM '{csv_input}'\n",
    "  FILEFORMAT = CSV\n",
    "  FORMAT_OPTIONS (\n",
    "    'header' = 'true',           -- O CSV tem cabeçalho\n",
    "    'inferSchema' = 'false',     -- Não inferi o schema do CSV\n",
    "    'enforceSchema' = 'false'    \n",
    "  )\n",
    "  COPY_OPTIONS (\n",
    "    'mergeSchema' = 'false'      -- Não fundi o schema, a tabela já tem o schema String\n",
    "  )\n",
    "\"\"\")\n",
    "\n",
    "print(f\"COPY INTO concluído para a tabela Delta '{tabela_bronze_caminho}'.\")\n",
    "print(\"Dados brutos ingeridos de forma robusta na Camada Bronze com todas as colunas como String.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "463d353b-5837-4d54-aae2-1be90c82ed7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verificando os dados na tabela bronze usando PySpark DataFrame API\n",
    "print(\"\\n--- Exemplo dos 5 primeiros registros na camada bronze (via PySpark DataFrame): ---\")\n",
    "bronze_df = spark.table(tabela_bronze_nome) \n",
    "display(bronze_df.limit(5))\n",
    "\n",
    "print(\"\\n--- Esquema REAL da tabela bronze (deve ser todo STRING): ---\")\n",
    "bronze_df.printSchema()\n",
    "\n",
    "print(\"\\n--- Contagem total de registros na camada bronze: ---\")\n",
    "final_record_count = bronze_df.count()\n",
    "print(f\"Total de registros na camada Bronze: {final_record_count}\")\n",
    "\n",
    "print(\"\\n--- Etapa 1 (Camada Bronze) concluída e verificada com sucesso! ---\")\n",
    "print(\"A tabela está pronta para a próxima etapa (Camada Silver).\")\n",
    "print(\"Para simular uma nova carga diária, adicione mais arquivos CSV ao Volume de entrada\")\n",
    "print(f\"('{csv_input}') e execute o notebook novamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b2d67ee-7f4c-4715-9462-e07e5b1b43d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Verificação adicional da tabela bronze via SQL (para confirmação)\n",
    "SELECT *\n",
    "FROM `workspace`.`default`.`fire_incidents_bronze`\n",
    "LIMIT 5;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7019969404573907,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1-Camada_Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
